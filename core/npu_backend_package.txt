[Skip to main content](https://intel.github.io/intel-npu-acceleration-library/python/<#main-content>) Back to top 
`Ctrl`+`K`
[ IntelÂ® NPU Acceleration Library documentation ](https://intel.github.io/intel-npu-acceleration-library/python/<../index.html>)
Search `Ctrl`+`K`
Library overview:
  * [Quickstart](https://intel.github.io/intel-npu-acceleration-library/python/<../index.html>)
  * [NPU overview](https://intel.github.io/intel-npu-acceleration-library/python/<../npu.html>)
  * [Basic usage](https://intel.github.io/intel-npu-acceleration-library/python/<../usage.html>)
  * [Advanced Setup](https://intel.github.io/intel-npu-acceleration-library/python/<../setup.html>)


Applications:
  * [Large Language models](https://intel.github.io/intel-npu-acceleration-library/python/<../llm.html>)
  * [Decoding LLM performance](https://intel.github.io/intel-npu-acceleration-library/python/<../llm_performance.html>)


Developements guide:
  * [Developer Guide](https://intel.github.io/intel-npu-acceleration-library/python/<../developer.html>)
  * [Adding New Operations in the Library](https://intel.github.io/intel-npu-acceleration-library/python/<../adding_operations.html>)


API Reference:
  * [Python API Reference](https://intel.github.io/intel-npu-acceleration-library/python/<intel_npu_acceleration_library.html>)
    * [intel_npu_acceleration_library.backend package](https://intel.github.io/intel-npu-acceleration-library/python/<#>)
    * [intel_npu_acceleration_library.nn package](https://intel.github.io/intel-npu-acceleration-library/python/<intel_npu_acceleration_library.nn.html>)
    * [intel_npu_acceleration_library.functional package](https://intel.github.io/intel-npu-acceleration-library/python/<intel_npu_acceleration_library.functional.html>)
  * [C++ API Reference](https://intel.github.io/intel-npu-acceleration-library/python/<../cpp_reference.html>)


  * [ .rst ](https://intel.github.io/intel-npu-acceleration-library/python/<../_sources/python/intel_npu_acceleration_library.backend.rst>)
  * .pdf


# intel_npu_acceleration_library.backend package
##  Contents 
  * [Submodules](https://intel.github.io/intel-npu-acceleration-library/python/<#submodules>)
  * [intel_npu_acceleration_library.backend.base module](https://intel.github.io/intel-npu-acceleration-library/python/<#intel-npu-acceleration-library-backend-base-module>)
    * `[BaseNPUBackend`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.base.BaseNPUBackend>)
      * `[BaseNPUBackend.save()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.base.BaseNPUBackend.save>)
      * `[BaseNPUBackend.saveCompiledModel()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.base.BaseNPUBackend.saveCompiledModel>)
    * `[BaseNPUBackendWithPrefetch`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.base.BaseNPUBackendWithPrefetch>)
      * `[BaseNPUBackendWithPrefetch.add_to_map()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.base.BaseNPUBackendWithPrefetch.add_to_map>)
      * `[BaseNPUBackendWithPrefetch.create_parameters()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.base.BaseNPUBackendWithPrefetch.create_parameters>)
      * `[BaseNPUBackendWithPrefetch.load_wt_fn()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.base.BaseNPUBackendWithPrefetch.load_wt_fn>)
      * `[BaseNPUBackendWithPrefetch.prefetchWeights()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.base.BaseNPUBackendWithPrefetch.prefetchWeights>)
      * `[BaseNPUBackendWithPrefetch.setWeights()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.base.BaseNPUBackendWithPrefetch.setWeights>)
    * `[adapt_weight()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.base.adapt_weight>)
  * [intel_npu_acceleration_library.backend.factory module](https://intel.github.io/intel-npu-acceleration-library/python/<#intel-npu-acceleration-library-backend-factory-module>)
    * `[NNFactory`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.factory.NNFactory>)
      * `[NNFactory.avg_pooling()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.factory.NNFactory.avg_pooling>)
      * `[NNFactory.compile()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.factory.NNFactory.compile>)
      * `[NNFactory.concat()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.factory.NNFactory.concat>)
      * `[NNFactory.constant()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.factory.NNFactory.constant>)
      * `[NNFactory.convolution()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.factory.NNFactory.convolution>)
      * `[NNFactory.get_backend_dtype()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.factory.NNFactory.get_backend_dtype>)
      * `[NNFactory.get_tensor_dtype()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.factory.NNFactory.get_tensor_dtype>)
      * `[NNFactory.get_tensor_recursively()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.factory.NNFactory.get_tensor_recursively>)
      * `[NNFactory.get_tensor_shape()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.factory.NNFactory.get_tensor_shape>)
      * `[NNFactory.linear()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.factory.NNFactory.linear>)
      * `[NNFactory.matmul()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.factory.NNFactory.matmul>)
      * `[NNFactory.max_pooling()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.factory.NNFactory.max_pooling>)
      * `[NNFactory.normL2()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.factory.NNFactory.normL2>)
      * `[NNFactory.parameter()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.factory.NNFactory.parameter>)
      * `[NNFactory.power()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.factory.NNFactory.power>)
      * `[NNFactory.reduce_max()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.factory.NNFactory.reduce_max>)
      * `[NNFactory.reduce_mean()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.factory.NNFactory.reduce_mean>)
      * `[NNFactory.reduce_min()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.factory.NNFactory.reduce_min>)
      * `[NNFactory.reduce_prod()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.factory.NNFactory.reduce_prod>)
      * `[NNFactory.reduce_sum()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.factory.NNFactory.reduce_sum>)
      * `[NNFactory.reshape()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.factory.NNFactory.reshape>)
      * `[NNFactory.return_tensor()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.factory.NNFactory.return_tensor>)
      * `[NNFactory.run()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.factory.NNFactory.run>)
      * `[NNFactory.set_input_tensor()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.factory.NNFactory.set_input_tensor>)
      * `[NNFactory.slice()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.factory.NNFactory.slice>)
      * `[NNFactory.to()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.factory.NNFactory.to>)
      * `[NNFactory.transpose()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.factory.NNFactory.transpose>)
      * `[NNFactory.unsqueeze()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.factory.NNFactory.unsqueeze>)
  * [intel_npu_acceleration_library.backend.linear module](https://intel.github.io/intel-npu-acceleration-library/python/<#intel-npu-acceleration-library-backend-linear-module>)
    * `[Linear`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.linear.Linear>)
      * `[Linear.run()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.linear.Linear.run>)
  * [intel_npu_acceleration_library.backend.matmul module](https://intel.github.io/intel-npu-acceleration-library/python/<#intel-npu-acceleration-library-backend-matmul-module>)
    * `[MatMul`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.matmul.MatMul>)
      * `[MatMul.run()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.matmul.MatMul.run>)
  * [intel_npu_acceleration_library.backend.mlp module](https://intel.github.io/intel-npu-acceleration-library/python/<#intel-npu-acceleration-library-backend-mlp-module>)
    * `[MLP`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.mlp.MLP>)
  * [intel_npu_acceleration_library.backend.qlinear module](https://intel.github.io/intel-npu-acceleration-library/python/<#intel-npu-acceleration-library-backend-qlinear-module>)
    * `[QLinear`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.qlinear.QLinear>)
      * `[QLinear.run()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.qlinear.QLinear.run>)
  * [intel_npu_acceleration_library.backend.qmatmul module](https://intel.github.io/intel-npu-acceleration-library/python/<#intel-npu-acceleration-library-backend-qmatmul-module>)
    * `[QMatMul`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.qmatmul.QMatMul>)
      * `[QMatMul.run()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.qmatmul.QMatMul.run>)
  * [intel_npu_acceleration_library.backend.runtime module](https://intel.github.io/intel-npu-acceleration-library/python/<#intel-npu-acceleration-library-backend-runtime-module>)
    * `[adapt_output_tensor()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.runtime.adapt_output_tensor>)
    * `[clear_cache()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.runtime.clear_cache>)
    * `[run_factory()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.runtime.run_factory>)
    * `[run_matmul()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.runtime.run_matmul>)
    * `[set_contiguous()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.runtime.set_contiguous>)
  * [Module contents](https://intel.github.io/intel-npu-acceleration-library/python/<#module-intel_npu_acceleration_library.backend>)
    * `[Convolution`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Convolution>)
    * `[Linear`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Linear>)
      * `[Linear.run()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Linear.run>)
    * `[MLP`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.MLP>)
    * `[MatMul`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.MatMul>)
      * `[MatMul.run()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.MatMul.run>)
    * `[NNFactory`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.NNFactory>)
      * `[NNFactory.avg_pooling()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.NNFactory.avg_pooling>)
      * `[NNFactory.compile()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.NNFactory.compile>)
      * `[NNFactory.concat()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.NNFactory.concat>)
      * `[NNFactory.constant()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.NNFactory.constant>)
      * `[NNFactory.convolution()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.NNFactory.convolution>)
      * `[NNFactory.get_backend_dtype()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.NNFactory.get_backend_dtype>)
      * `[NNFactory.get_tensor_dtype()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.NNFactory.get_tensor_dtype>)
      * `[NNFactory.get_tensor_recursively()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.NNFactory.get_tensor_recursively>)
      * `[NNFactory.get_tensor_shape()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.NNFactory.get_tensor_shape>)
      * `[NNFactory.linear()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.NNFactory.linear>)
      * `[NNFactory.matmul()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.NNFactory.matmul>)
      * `[NNFactory.max_pooling()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.NNFactory.max_pooling>)
      * `[NNFactory.normL2()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.NNFactory.normL2>)
      * `[NNFactory.parameter()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.NNFactory.parameter>)
      * `[NNFactory.power()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.NNFactory.power>)
      * `[NNFactory.reduce_max()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.NNFactory.reduce_max>)
      * `[NNFactory.reduce_mean()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.NNFactory.reduce_mean>)
      * `[NNFactory.reduce_min()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.NNFactory.reduce_min>)
      * `[NNFactory.reduce_prod()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.NNFactory.reduce_prod>)
      * `[NNFactory.reduce_sum()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.NNFactory.reduce_sum>)
      * `[NNFactory.reshape()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.NNFactory.reshape>)
      * `[NNFactory.return_tensor()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.NNFactory.return_tensor>)
      * `[NNFactory.run()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.NNFactory.run>)
      * `[NNFactory.set_input_tensor()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.NNFactory.set_input_tensor>)
      * `[NNFactory.slice()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.NNFactory.slice>)
      * `[NNFactory.to()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.NNFactory.to>)
      * `[NNFactory.transpose()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.NNFactory.transpose>)
      * `[NNFactory.unsqueeze()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.NNFactory.unsqueeze>)
    * `[QLinear`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.QLinear>)
      * `[QLinear.run()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.QLinear.run>)
    * `[QMatMul`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.QMatMul>)
      * `[QMatMul.run()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.QMatMul.run>)
    * `[SDPA`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.SDPA>)
      * `[SDPA.run()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.SDPA.run>)
    * `[SimpleSDPA`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.SimpleSDPA>)
      * `[SimpleSDPA.run()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.SimpleSDPA.run>)
    * `[Tensor`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor>)
      * `[Tensor.__add__()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.__add__>)
      * `[Tensor.__sub__()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.__sub__>)
      * `[Tensor.__mul__()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.__mul__>)
      * `[Tensor.__truediv__()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.__truediv__>)
      * `[Tensor.__neg__()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.__neg__>)
      * `[Tensor.__repr__()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.__repr__>)
      * `[Tensor.__str__()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.__str__>)
      * `[Tensor.__len__()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.__len__>)
      * `[Tensor.T()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.T>)
      * `[Tensor.squeeze()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.squeeze>)
      * `[Tensor.unsqueeze()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.unsqueeze>)
      * `[Tensor.__matmul__()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.__matmul__>)
      * `[Tensor.acos()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.acos>)
      * `[Tensor.asin()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.asin>)
      * `[Tensor.atan()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.atan>)
      * `[Tensor.acosh()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.acosh>)
      * `[Tensor.asinh()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.asinh>)
      * `[Tensor.atanh()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.atanh>)
      * `[Tensor.cosh()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.cosh>)
      * `[Tensor.sinh()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.sinh>)
      * `[Tensor.tanh()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.tanh>)
      * `[Tensor.cos()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.cos>)
      * `[Tensor.sin()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.sin>)
      * `[Tensor.tan()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.tan>)
      * `[Tensor.ceiling()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.ceiling>)
      * `[Tensor.clamp()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.clamp>)
      * `[Tensor.elu()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.elu>)
      * `[Tensor.erf()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.erf>)
      * `[Tensor.exp()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.exp>)
      * `[Tensor.floor()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.floor>)
      * `[Tensor.grn()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.grn>)
      * `[Tensor.hsigmoid()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.hsigmoid>)
      * `[Tensor.hswish()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.hswish>)
      * `[Tensor.log()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.log>)
      * `[Tensor.mish()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.mish>)
      * `[Tensor.relu()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.relu>)
      * `[Tensor.round()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.round>)
      * `[Tensor.sigmoid()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.sigmoid>)
      * `[Tensor.sign()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.sign>)
      * `[Tensor.softmax()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.softmax>)
      * `[Tensor.softplus()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.softplus>)
      * `[Tensor.sqrt()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.sqrt>)
      * `[Tensor.max()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.max>)
      * `[Tensor.mean()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.mean>)
      * `[Tensor.min()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.min>)
      * `[Tensor.prod()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.prod>)
      * `[Tensor.sum()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.sum>)
      * `[Tensor.T`](https://intel.github.io/intel-npu-acceleration-library/python/<#id0>)
      * `[Tensor.acos()`](https://intel.github.io/intel-npu-acceleration-library/python/<#id1>)
      * `[Tensor.acosh()`](https://intel.github.io/intel-npu-acceleration-library/python/<#id2>)
      * `[Tensor.asin()`](https://intel.github.io/intel-npu-acceleration-library/python/<#id3>)
      * `[Tensor.asinh()`](https://intel.github.io/intel-npu-acceleration-library/python/<#id4>)
      * `[Tensor.atan()`](https://intel.github.io/intel-npu-acceleration-library/python/<#id5>)
      * `[Tensor.atanh()`](https://intel.github.io/intel-npu-acceleration-library/python/<#id6>)
      * `[Tensor.ceiling()`](https://intel.github.io/intel-npu-acceleration-library/python/<#id7>)
      * `[Tensor.chunk()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.chunk>)
      * `[Tensor.clamp()`](https://intel.github.io/intel-npu-acceleration-library/python/<#id8>)
      * `[Tensor.cos()`](https://intel.github.io/intel-npu-acceleration-library/python/<#id9>)
      * `[Tensor.cosh()`](https://intel.github.io/intel-npu-acceleration-library/python/<#id10>)
      * `[Tensor.dim()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.dim>)
      * `[Tensor.dtype`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.dtype>)
      * `[Tensor.elu()`](https://intel.github.io/intel-npu-acceleration-library/python/<#id11>)
      * `[Tensor.erf()`](https://intel.github.io/intel-npu-acceleration-library/python/<#id12>)
      * `[Tensor.exp()`](https://intel.github.io/intel-npu-acceleration-library/python/<#id13>)
      * `[Tensor.factory`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.factory>)
      * `[Tensor.flatten()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.flatten>)
      * `[Tensor.floor()`](https://intel.github.io/intel-npu-acceleration-library/python/<#id14>)
      * `[Tensor.grn()`](https://intel.github.io/intel-npu-acceleration-library/python/<#id15>)
      * `[Tensor.hsigmoid()`](https://intel.github.io/intel-npu-acceleration-library/python/<#id16>)
      * `[Tensor.hswish()`](https://intel.github.io/intel-npu-acceleration-library/python/<#id17>)
      * `[Tensor.log()`](https://intel.github.io/intel-npu-acceleration-library/python/<#id18>)
      * `[Tensor.max()`](https://intel.github.io/intel-npu-acceleration-library/python/<#id19>)
      * `[Tensor.mean()`](https://intel.github.io/intel-npu-acceleration-library/python/<#id20>)
      * `[Tensor.min()`](https://intel.github.io/intel-npu-acceleration-library/python/<#id21>)
      * `[Tensor.mish()`](https://intel.github.io/intel-npu-acceleration-library/python/<#id22>)
      * `[Tensor.node`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.node>)
      * `[Tensor.permute()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.permute>)
      * `[Tensor.prod()`](https://intel.github.io/intel-npu-acceleration-library/python/<#id23>)
      * `[Tensor.relu()`](https://intel.github.io/intel-npu-acceleration-library/python/<#id24>)
      * `[Tensor.reshape()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.reshape>)
      * `[Tensor.round()`](https://intel.github.io/intel-npu-acceleration-library/python/<#id25>)
      * `[Tensor.shape`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.shape>)
      * `[Tensor.sigmoid()`](https://intel.github.io/intel-npu-acceleration-library/python/<#id26>)
      * `[Tensor.sign()`](https://intel.github.io/intel-npu-acceleration-library/python/<#id27>)
      * `[Tensor.sin()`](https://intel.github.io/intel-npu-acceleration-library/python/<#id28>)
      * `[Tensor.sinh()`](https://intel.github.io/intel-npu-acceleration-library/python/<#id29>)
      * `[Tensor.size()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.size>)
      * `[Tensor.softmax()`](https://intel.github.io/intel-npu-acceleration-library/python/<#id30>)
      * `[Tensor.softplus()`](https://intel.github.io/intel-npu-acceleration-library/python/<#id31>)
      * `[Tensor.sqrt()`](https://intel.github.io/intel-npu-acceleration-library/python/<#id32>)
      * `[Tensor.squeeze()`](https://intel.github.io/intel-npu-acceleration-library/python/<#id33>)
      * `[Tensor.sum()`](https://intel.github.io/intel-npu-acceleration-library/python/<#id34>)
      * `[Tensor.tan()`](https://intel.github.io/intel-npu-acceleration-library/python/<#id35>)
      * `[Tensor.tanh()`](https://intel.github.io/intel-npu-acceleration-library/python/<#id36>)
      * `[Tensor.to()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.to>)
      * `[Tensor.transpose()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.transpose>)
      * `[Tensor.type()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.type>)
      * `[Tensor.unsqueeze()`](https://intel.github.io/intel-npu-acceleration-library/python/<#id37>)
      * `[Tensor.view()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.view>)
    * `[clear_cache()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.clear_cache>)
    * `[get_driver_version()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.get_driver_version>)
    * `[npu_available()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.npu_available>)
    * `[run_factory()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.run_factory>)
    * `[run_matmul()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.run_matmul>)


# intel_npu_acceleration_library.backend package[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel-npu-acceleration-library-backend-package> "Link to this heading")
## Submodules[#](https://intel.github.io/intel-npu-acceleration-library/python/<#submodules> "Link to this heading")
## intel_npu_acceleration_library.backend.base module[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel-npu-acceleration-library-backend-base-module> "Link to this heading")
_class_ intel_npu_acceleration_library.backend.base.BaseNPUBackend(_profile :bool|None=False_)[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.base.BaseNPUBackend> "Link to this definition")
    
Bases: `object`
A base class that represent a abstract Matrix-Matrix operation on the NPU.
save(_path :str_)[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.base.BaseNPUBackend.save> "Link to this definition")
    
Save the Openvino model.
Parameters:
    
**path** (_str_) â the model save path
saveCompiledModel(_path :str_)[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.base.BaseNPUBackend.saveCompiledModel> "Link to this definition")
    
Save the compiled model.
Parameters:
    
**path** (_str_) â the compiled model save path
_class_ intel_npu_acceleration_library.backend.base.BaseNPUBackendWithPrefetch(_profile :bool_)[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.base.BaseNPUBackendWithPrefetch> "Link to this definition")
    
Bases: `[BaseNPUBackend`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.base.BaseNPUBackend> "intel_npu_acceleration_library.backend.base.BaseNPUBackend")
A base class that represent a abstract Matrix-Matrix operation on the NPU.
Linear type classes employ an algorithm to optimize weights prefetching
add_to_map(_wt_hash :str_, _weights :Iterable[ndarray|Tuple[ndarray,...]]_)[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.base.BaseNPUBackendWithPrefetch.add_to_map> "Link to this definition")
    
Add an operation parameters to the operation hash:parameter map.
Parameters:
    
  * **wt_hash** (_str_) â operation hash
  * **weights** (_Iterable_ _[__Union_ _[__np.ndarray_ _,__Tuple_ _[__np.ndarray_ _,__...__]__]__]_) â Operation parameters


create_parameters(_weights :Iterable[ndarray|Tuple[ndarray,...]]_) â _Pointer[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.base.BaseNPUBackendWithPrefetch.create_parameters> "Link to this definition")
    
Create an operation parameter from a list of weights.
Parameters:
    
**weights** (_Iterable_ _[__Union_ _[__np.ndarray_ _,__Tuple_ _[__np.ndarray_ _,__...__]__]__]_) â Operation parameters
Raises:
    
  * **RuntimeError** â Quantized weights needs to be in int8 format
  * **ValueError** â Invalid dtype for scale


Returns:
    
an instance to the Parameters object
Return type:
    
ctypes._Pointer
load_wt_fn(_module_ , _parameters_)[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.base.BaseNPUBackendWithPrefetch.load_wt_fn> "Link to this definition")
    
Load asyncronously the parameter into the NPU.
Parameters:
    
  * **module** â the NPU backend module
  * **parameters** â the weights parameter class


prefetchWeights()[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.base.BaseNPUBackendWithPrefetch.prefetchWeights> "Link to this definition")
    
Prefetch next operation weights.
setWeights(_wt_hash :str|None_, _* args:ndarray|Tuple[ndarray,...]_) â bool[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.base.BaseNPUBackendWithPrefetch.setWeights> "Link to this definition")
    
Set the operation weights in the NPU.
Parameters:
    
  * **wt_hash** (_str_) â operation hash. If set to None force the load of the weights
  * **args** (_Union_ _[__np.ndarray_ _,__Tuple_ _[__np.ndarray_ _,__...__]__]_) â Variable length weights list. Can be a np array or a tuple of weight, scale in case of quantized tensors


Returns:
    
Return True if the op parameters are already in the op map
Return type:
    
bool
intel_npu_acceleration_library.backend.base.adapt_weight(_w :ndarray_) â ndarray[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.base.adapt_weight> "Link to this definition")
    
Adapt the weights to run on the NPU.
Parameters:
    
**w** (_np.ndarray_) â weights array
Returns:
    
The adapted array
Return type:
    
np.ndarray
## intel_npu_acceleration_library.backend.factory module[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel-npu-acceleration-library-backend-factory-module> "Link to this heading")
_class_ intel_npu_acceleration_library.backend.factory.NNFactory(_profile :bool=False_, _device :str='NPU'_)[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.factory.NNFactory> "Link to this definition")
    
Bases: `[BaseNPUBackendWithPrefetch`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.base.BaseNPUBackendWithPrefetch> "intel_npu_acceleration_library.backend.base.BaseNPUBackendWithPrefetch")
Linear class, computing a matrix matrix multiplication with weights prefetching.
avg_pooling(_* args:Any_, _** kwargs:Any_) â [Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.tensor.Tensor")[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.factory.NNFactory.avg_pooling> "Link to this definition")
    
Wrap the output of a function in a Tensor object.
Parameters:
    
  * **args** (_Any_) â Variable length argument list
  * **kwargs** (_Any_) â Arbitrary keyword arguments


Returns:
    
Tensor object
Return type:
    
[Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.Tensor")
compile()[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.factory.NNFactory.compile> "Link to this definition")
    
Finalize and compile a model.
concat(_* args:Any_, _** kwargs:Any_) â [Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.tensor.Tensor")[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.factory.NNFactory.concat> "Link to this definition")
    
Wrap the output of a function in a Tensor object.
Parameters:
    
  * **args** (_Any_) â Variable length argument list
  * **kwargs** (_Any_) â Arbitrary keyword arguments


Returns:
    
Tensor object
Return type:
    
[Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.Tensor")
constant(_* args:Any_, _** kwargs:Any_) â [Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.tensor.Tensor")[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.factory.NNFactory.constant> "Link to this definition")
    
Wrap the output of a function in a Tensor object.
Parameters:
    
  * **args** (_Any_) â Variable length argument list
  * **kwargs** (_Any_) â Arbitrary keyword arguments


Returns:
    
Tensor object
Return type:
    
[Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.Tensor")
convolution(_* args:Any_, _** kwargs:Any_) â [Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.tensor.Tensor")[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.factory.NNFactory.convolution> "Link to this definition")
    
Wrap the output of a function in a Tensor object.
Parameters:
    
  * **args** (_Any_) â Variable length argument list
  * **kwargs** (_Any_) â Arbitrary keyword arguments


Returns:
    
Tensor object
Return type:
    
[Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.Tensor")
get_backend_dtype(_dtype_) â c_char_p[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.factory.NNFactory.get_backend_dtype> "Link to this definition")
    
Get the string representation of the dtype.
Parameters:
    
**dtype** â numpy dtype
Raises:
    
**RuntimeError** â Unsupported datatype
Returns:
    
string representation of the dtype
Return type:
    
ctypes.c_char_p
get_tensor_dtype(_node_)[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.factory.NNFactory.get_tensor_dtype> "Link to this definition")
    
Get tensor dtype.
Parameters:
    
**node** â network node
Raises:
    
**RuntimeError** â Unsupported dtype
Returns:
    
tensor dtype
Return type:
    
str
get_tensor_recursively(_args :Sequence[Any]_) â List[ndarray][#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.factory.NNFactory.get_tensor_recursively> "Link to this definition")
    
Get tensor recursively for a list of arguments.
Parameters:
    
**args** (_Sequence_ _[__Any_ _]_) â Sequence of tensors, tuple of tensors and additional arguments
Returns:
    
Sequence of tensors
Return type:
    
List[np.ndarray]
get_tensor_shape(_node_)[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.factory.NNFactory.get_tensor_shape> "Link to this definition")
    
Get tensor shape.
Parameters:
    
**node** â network node
Returns:
    
tensor shape
Return type:
    
tuple[int]
linear(_* args:Any_, _** kwargs:Any_) â [Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.tensor.Tensor")[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.factory.NNFactory.linear> "Link to this definition")
    
Wrap the output of a function in a Tensor object.
Parameters:
    
  * **args** (_Any_) â Variable length argument list
  * **kwargs** (_Any_) â Arbitrary keyword arguments


Returns:
    
Tensor object
Return type:
    
[Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.Tensor")
matmul(_* args:Any_, _** kwargs:Any_) â [Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.tensor.Tensor")[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.factory.NNFactory.matmul> "Link to this definition")
    
Wrap the output of a function in a Tensor object.
Parameters:
    
  * **args** (_Any_) â Variable length argument list
  * **kwargs** (_Any_) â Arbitrary keyword arguments


Returns:
    
Tensor object
Return type:
    
[Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.Tensor")
max_pooling(_* args:Any_, _** kwargs:Any_) â [Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.tensor.Tensor")[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.factory.NNFactory.max_pooling> "Link to this definition")
    
Wrap the output of a function in a Tensor object.
Parameters:
    
  * **args** (_Any_) â Variable length argument list
  * **kwargs** (_Any_) â Arbitrary keyword arguments


Returns:
    
Tensor object
Return type:
    
[Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.Tensor")
normL2(_* args:Any_, _** kwargs:Any_) â [Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.tensor.Tensor")[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.factory.NNFactory.normL2> "Link to this definition")
    
Wrap the output of a function in a Tensor object.
Parameters:
    
  * **args** (_Any_) â Variable length argument list
  * **kwargs** (_Any_) â Arbitrary keyword arguments


Returns:
    
Tensor object
Return type:
    
[Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.Tensor")
parameter(_* args:Any_, _** kwargs:Any_) â [Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.tensor.Tensor")[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.factory.NNFactory.parameter> "Link to this definition")
    
Wrap the output of a function in a Tensor object.
Parameters:
    
  * **args** (_Any_) â Variable length argument list
  * **kwargs** (_Any_) â Arbitrary keyword arguments


Returns:
    
Tensor object
Return type:
    
[Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.Tensor")
power(_* args:Any_, _** kwargs:Any_) â [Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.tensor.Tensor")[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.factory.NNFactory.power> "Link to this definition")
    
Wrap the output of a function in a Tensor object.
Parameters:
    
  * **args** (_Any_) â Variable length argument list
  * **kwargs** (_Any_) â Arbitrary keyword arguments


Returns:
    
Tensor object
Return type:
    
[Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.Tensor")
reduce_max(_* args:Any_, _** kwargs:Any_) â [Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.tensor.Tensor")[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.factory.NNFactory.reduce_max> "Link to this definition")
    
Wrap the output of a function in a Tensor object.
Parameters:
    
  * **args** (_Any_) â Variable length argument list
  * **kwargs** (_Any_) â Arbitrary keyword arguments


Returns:
    
Tensor object
Return type:
    
[Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.Tensor")
reduce_mean(_* args:Any_, _** kwargs:Any_) â [Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.tensor.Tensor")[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.factory.NNFactory.reduce_mean> "Link to this definition")
    
Wrap the output of a function in a Tensor object.
Parameters:
    
  * **args** (_Any_) â Variable length argument list
  * **kwargs** (_Any_) â Arbitrary keyword arguments


Returns:
    
Tensor object
Return type:
    
[Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.Tensor")
reduce_min(_* args:Any_, _** kwargs:Any_) â [Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.tensor.Tensor")[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.factory.NNFactory.reduce_min> "Link to this definition")
    
Wrap the output of a function in a Tensor object.
Parameters:
    
  * **args** (_Any_) â Variable length argument list
  * **kwargs** (_Any_) â Arbitrary keyword arguments


Returns:
    
Tensor object
Return type:
    
[Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.Tensor")
reduce_prod(_* args:Any_, _** kwargs:Any_) â [Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.tensor.Tensor")[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.factory.NNFactory.reduce_prod> "Link to this definition")
    
Wrap the output of a function in a Tensor object.
Parameters:
    
  * **args** (_Any_) â Variable length argument list
  * **kwargs** (_Any_) â Arbitrary keyword arguments


Returns:
    
Tensor object
Return type:
    
[Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.Tensor")
reduce_sum(_* args:Any_, _** kwargs:Any_) â [Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.tensor.Tensor")[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.factory.NNFactory.reduce_sum> "Link to this definition")
    
Wrap the output of a function in a Tensor object.
Parameters:
    
  * **args** (_Any_) â Variable length argument list
  * **kwargs** (_Any_) â Arbitrary keyword arguments


Returns:
    
Tensor object
Return type:
    
[Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.Tensor")
reshape(_* args:Any_, _** kwargs:Any_) â [Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.tensor.Tensor")[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.factory.NNFactory.reshape> "Link to this definition")
    
Wrap the output of a function in a Tensor object.
Parameters:
    
  * **args** (_Any_) â Variable length argument list
  * **kwargs** (_Any_) â Arbitrary keyword arguments


Returns:
    
Tensor object
Return type:
    
[Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.Tensor")
return_tensor() â F[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.factory.NNFactory.return_tensor> "Link to this definition")
    
Wrap the output of a function in a Tensor object.
Parameters:
    
**fn** (_function_) â Function
Returns:
    
A function that wraps the output in a Tensor object
Return type:
    
function
run(_X :ndarray_, _* weights:ndarray|Tuple[ndarray,ndarray]_, _** kwargs:Any_) â ndarray[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.factory.NNFactory.run> "Link to this definition")
    
Run the layer: X * W^T.
Parameters:
    
  * **X** (_np.ndarray_) â lhs operator
  * **weights** (_Union_ _[__np.ndarray_ _,__Tuple_ _[__np.ndarray_ _,__np.ndarray_ _]__]_) â rhs operators
  * **kwargs** (_Any_) â additional arguments


Returns:
    
result
Return type:
    
np.ndarray
set_input_tensor(_tensor :ndarray_, _idx :int_)[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.factory.NNFactory.set_input_tensor> "Link to this definition")
    
Set input tensor.
Parameters:
    
  * **tensor** (_np.ndarray_) â Input tensor
  * **idx** (_int_) â tensor index


slice(_* args:Any_, _** kwargs:Any_) â [Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.tensor.Tensor")[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.factory.NNFactory.slice> "Link to this definition")
    
Wrap the output of a function in a Tensor object.
Parameters:
    
  * **args** (_Any_) â Variable length argument list
  * **kwargs** (_Any_) â Arbitrary keyword arguments


Returns:
    
Tensor object
Return type:
    
[Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.Tensor")
to(_* args:Any_, _** kwargs:Any_) â [Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.tensor.Tensor")[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.factory.NNFactory.to> "Link to this definition")
    
Wrap the output of a function in a Tensor object.
Parameters:
    
  * **args** (_Any_) â Variable length argument list
  * **kwargs** (_Any_) â Arbitrary keyword arguments


Returns:
    
Tensor object
Return type:
    
[Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.Tensor")
transpose(_* args:Any_, _** kwargs:Any_) â [Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.tensor.Tensor")[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.factory.NNFactory.transpose> "Link to this definition")
    
Wrap the output of a function in a Tensor object.
Parameters:
    
  * **args** (_Any_) â Variable length argument list
  * **kwargs** (_Any_) â Arbitrary keyword arguments


Returns:
    
Tensor object
Return type:
    
[Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.Tensor")
unsqueeze(_* args:Any_, _** kwargs:Any_) â [Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.tensor.Tensor")[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.factory.NNFactory.unsqueeze> "Link to this definition")
    
Wrap the output of a function in a Tensor object.
Parameters:
    
  * **args** (_Any_) â Variable length argument list
  * **kwargs** (_Any_) â Arbitrary keyword arguments


Returns:
    
Tensor object
Return type:
    
[Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.Tensor")
## intel_npu_acceleration_library.backend.linear module[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel-npu-acceleration-library-backend-linear-module> "Link to this heading")
_class_ intel_npu_acceleration_library.backend.linear.Linear(_inC :int_, _outC :int_, _batch :int_, _profile :bool=False_, _device :str='NPU'_)[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.linear.Linear> "Link to this definition")
    
Bases: `[NNFactory`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.factory.NNFactory> "intel_npu_acceleration_library.backend.factory.NNFactory")
Linear class, computing a matrix matrix multiplication with weights prefetching.
run(_X :ndarray_, _W :ndarray_, _op_id :str_) â ndarray[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.linear.Linear.run> "Link to this definition")
    
Run the layer: X * W^T.
Parameters:
    
  * **X** (_np.ndarray_) â lhs operator
  * **W** (_np.ndarray_) â rhs operator
  * **op_id** (_str_) â operation id


Raises:
    
**RuntimeError** â Input or weight tensor shape mismatch
Returns:
    
result
Return type:
    
np.ndarray
## intel_npu_acceleration_library.backend.matmul module[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel-npu-acceleration-library-backend-matmul-module> "Link to this heading")
_class_ intel_npu_acceleration_library.backend.matmul.MatMul(_inC :int_, _outC :int_, _batch :int_, _profile :bool=False_, _device :str='NPU'_)[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.matmul.MatMul> "Link to this definition")
    
Bases: `[NNFactory`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.factory.NNFactory> "intel_npu_acceleration_library.backend.factory.NNFactory")
MatMul class, computing a matrix matrix multiplication.
run(_X :ndarray_, _W :ndarray_) â ndarray[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.matmul.MatMul.run> "Link to this definition")
    
Run the layer: X * W^T.
Parameters:
    
  * **X** (_np.ndarray_) â lhs operator
  * **W** (_np.ndarray_) â rhs operator


Raises:
    
**RuntimeError** â Input or weight tensor shape mismatch
Returns:
    
result
Return type:
    
np.ndarray
## intel_npu_acceleration_library.backend.mlp module[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel-npu-acceleration-library-backend-mlp-module> "Link to this heading")
_class_ intel_npu_acceleration_library.backend.mlp.MLP(_input_shape :Sequence[int]_, _intermediate_size :int_, _activation :str='swiglu'_, _bias :bool|None=False_, _profile :bool=False_, _device :str='NPU'_, _** additional_args_)[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.mlp.MLP> "Link to this definition")
    
Bases: `[NNFactory`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.factory.NNFactory> "intel_npu_acceleration_library.backend.factory.NNFactory")
Linear class, computing a matrix matrix multiplication with weights prefetching.
## intel_npu_acceleration_library.backend.qlinear module[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel-npu-acceleration-library-backend-qlinear-module> "Link to this heading")
_class_ intel_npu_acceleration_library.backend.qlinear.QLinear(_inC: int_, _outC: int_, _batch: int_, _profile: bool = False_, _device: str = 'NPU'_, _dtype: ~numpy.dtype = <class 'numpy.int8'>_)[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.qlinear.QLinear> "Link to this definition")
    
Bases: `[NNFactory`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.factory.NNFactory> "intel_npu_acceleration_library.backend.factory.NNFactory")
Quantized Linear class, computing a matrix matrix multiplication with weights prefetching.
run(_X :ndarray_, _W :ndarray_, _scale :ndarray_, _op_id :str_) â ndarray[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.qlinear.QLinear.run> "Link to this definition")
    
Run the layer: $X * (W * S)^T$ .
Parameters:
    
  * **X** (_np.ndarray_) â activation
  * **W** (_np.ndarray_) â quantized weights
  * **scale** (_np.ndarray_) â quantization scale
  * **op_id** (_str_) â operation id


Raises:
    
**RuntimeError** â Input, weights or scale shape mismatch
Returns:
    
result
Return type:
    
np.ndarray
## intel_npu_acceleration_library.backend.qmatmul module[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel-npu-acceleration-library-backend-qmatmul-module> "Link to this heading")
_class_ intel_npu_acceleration_library.backend.qmatmul.QMatMul(_inC: int_, _outC: int_, _batch: int_, _profile: bool = False_, _device: str = 'NPU'_, _dtype: ~numpy.dtype = <class 'numpy.int8'>_)[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.qmatmul.QMatMul> "Link to this definition")
    
Bases: `[NNFactory`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.factory.NNFactory> "intel_npu_acceleration_library.backend.factory.NNFactory")
Quantized Linear class, computing a matrix matrix multiplication.
run(_X :ndarray_, _W :ndarray_, _scale :ndarray_) â ndarray[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.qmatmul.QMatMul.run> "Link to this definition")
    
Run the layer: X * (W * S)^T.
Parameters:
    
  * **X** (_np.ndarray_) â activation
  * **W** (_np.ndarray_) â quantized weights
  * **scale** (_np.ndarray_) â quantization scale


Raises:
    
**RuntimeError** â Input, weights or scale shape mismatch
Returns:
    
result
Return type:
    
np.ndarray
## intel_npu_acceleration_library.backend.runtime module[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel-npu-acceleration-library-backend-runtime-module> "Link to this heading")
intel_npu_acceleration_library.backend.runtime.adapt_output_tensor(_output :ndarray_, _original_shape :Size_, _input_dtype :dtype_) â Tensor[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.runtime.adapt_output_tensor> "Link to this definition")
    
Adapt the output tensor to the original shape and dtype.
Parameters:
    
  * **output** (_np.ndarray_) â output tensor
  * **original_shape** (_torch.Size_) â original shape
  * **input_dtype** (_torch.dtype_) â input dtype


Returns:
    
output tensor
Return type:
    
torch.Tensor
intel_npu_acceleration_library.backend.runtime.clear_cache()[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.runtime.clear_cache> "Link to this definition")
    
Clear the cache of models.
intel_npu_acceleration_library.backend.runtime.run_factory(_x :Tensor|List[Tensor]_, _weights :List[Tensor]_, _backend_cls :Any_, _op_id :str|None=None_) â Tensor[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.runtime.run_factory> "Link to this definition")
    
Run a factory operation. Depending on the datatype of the weights it runs a float or quantized operation.
Parameters:
    
  * **x** (_Union_ _[__torch.Tensor_ _,__List_ _[__torch.Tensor_ _]__]_) â Activation tensor(s). Its dtype must be torch.float16
  * **weights** (_torch.Tensor_) â Weights tensor. Its dtype can be torch.float16 or torch.int8
  * **backend_cls** (_Any_) â Backend class to run
  * **op_id** (_Optional_ _[__str_ _]__,__optional_) â Operation ID. Defaults to None.


Returns:
    
result
Return type:
    
torch.Tensor
intel_npu_acceleration_library.backend.runtime.run_matmul(_x :Tensor_, _weights :Tensor_, _scale :Tensor|None=None_, _op_id :str|None=None_) â Tensor[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.runtime.run_matmul> "Link to this definition")
    
Run a matmul operation. Depending on the datatype of the weights it runs a float or quantized operation.
Parameters:
    
  * **x** (_torch.Tensor_) â Activation tensor. Its dtype must be torch.float16
  * **weights** (_torch.Tensor_) â Weights tensor. Its dtype can be torch.float16 or torch.int8
  * **scale** (_Optional_ _[__torch.Tensor_ _]__,__optional_) â Quantization scale. If weights.dtype == torch.int8 then it must be set. Defaults to None.
  * **op_id** (_Optional_ _[__str_ _]__,__optional_) â Operation ID. Defaults to None.


Raises:
    
**RuntimeError** â Unsupported weights datatype. Supported types: [torch.float16, torch.int8]
Returns:
    
result
Return type:
    
torch.Tensor
intel_npu_acceleration_library.backend.runtime.set_contiguous(_tensor :Tensor_) â Tensor[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.runtime.set_contiguous> "Link to this definition")
    
Set tensor to be contiguous in memory.
Parameters:
    
**tensor** (_torch.Tensor_) â input tensor
Returns:
    
output, contiguous tensor
Return type:
    
torch.Tensor
## Module contents[#](https://intel.github.io/intel-npu-acceleration-library/python/<#module-intel_npu_acceleration_library.backend> "Link to this heading")
_class_ intel_npu_acceleration_library.backend.Convolution(_input_shape :Sequence[int]_, _weights_shape :Sequence[int]_, _bias :bool=False_, _strides :int|Sequence[int]=1_, _padding :int|Sequence[int]=0_, _dilation :int|Sequence[int]=1_, _groups :int=1_, _profile :bool=False_, _device :str='NPU'_)[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Convolution> "Link to this definition")
    
Bases: `[NNFactory`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.factory.NNFactory> "intel_npu_acceleration_library.backend.factory.NNFactory")
Linear class, computing a matrix matrix multiplication with weights prefetching.
_class_ intel_npu_acceleration_library.backend.Linear(_inC :int_, _outC :int_, _batch :int_, _profile :bool=False_, _device :str='NPU'_)[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Linear> "Link to this definition")
    
Bases: `[NNFactory`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.factory.NNFactory> "intel_npu_acceleration_library.backend.factory.NNFactory")
Linear class, computing a matrix matrix multiplication with weights prefetching.
run(_X :ndarray_, _W :ndarray_, _op_id :str_) â ndarray[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Linear.run> "Link to this definition")
    
Run the layer: X * W^T.
Parameters:
    
  * **X** (_np.ndarray_) â lhs operator
  * **W** (_np.ndarray_) â rhs operator
  * **op_id** (_str_) â operation id


Raises:
    
**RuntimeError** â Input or weight tensor shape mismatch
Returns:
    
result
Return type:
    
np.ndarray
_class_ intel_npu_acceleration_library.backend.MLP(_input_shape :Sequence[int]_, _intermediate_size :int_, _activation :str='swiglu'_, _bias :bool|None=False_, _profile :bool=False_, _device :str='NPU'_, _** additional_args_)[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.MLP> "Link to this definition")
    
Bases: `[NNFactory`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.factory.NNFactory> "intel_npu_acceleration_library.backend.factory.NNFactory")
Linear class, computing a matrix matrix multiplication with weights prefetching.
_class_ intel_npu_acceleration_library.backend.MatMul(_inC :int_, _outC :int_, _batch :int_, _profile :bool=False_, _device :str='NPU'_)[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.MatMul> "Link to this definition")
    
Bases: `[NNFactory`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.factory.NNFactory> "intel_npu_acceleration_library.backend.factory.NNFactory")
MatMul class, computing a matrix matrix multiplication.
run(_X :ndarray_, _W :ndarray_) â ndarray[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.MatMul.run> "Link to this definition")
    
Run the layer: X * W^T.
Parameters:
    
  * **X** (_np.ndarray_) â lhs operator
  * **W** (_np.ndarray_) â rhs operator


Raises:
    
**RuntimeError** â Input or weight tensor shape mismatch
Returns:
    
result
Return type:
    
np.ndarray
_class_ intel_npu_acceleration_library.backend.NNFactory(_profile :bool=False_, _device :str='NPU'_)[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.NNFactory> "Link to this definition")
    
Bases: `[BaseNPUBackendWithPrefetch`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.base.BaseNPUBackendWithPrefetch> "intel_npu_acceleration_library.backend.base.BaseNPUBackendWithPrefetch")
Linear class, computing a matrix matrix multiplication with weights prefetching.
avg_pooling(_* args:Any_, _** kwargs:Any_) â [Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.tensor.Tensor")[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.NNFactory.avg_pooling> "Link to this definition")
    
Wrap the output of a function in a Tensor object.
Parameters:
    
  * **args** (_Any_) â Variable length argument list
  * **kwargs** (_Any_) â Arbitrary keyword arguments


Returns:
    
Tensor object
Return type:
    
[Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.Tensor")
compile()[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.NNFactory.compile> "Link to this definition")
    
Finalize and compile a model.
concat(_* args:Any_, _** kwargs:Any_) â [Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.tensor.Tensor")[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.NNFactory.concat> "Link to this definition")
    
Wrap the output of a function in a Tensor object.
Parameters:
    
  * **args** (_Any_) â Variable length argument list
  * **kwargs** (_Any_) â Arbitrary keyword arguments


Returns:
    
Tensor object
Return type:
    
[Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.Tensor")
constant(_* args:Any_, _** kwargs:Any_) â [Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.tensor.Tensor")[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.NNFactory.constant> "Link to this definition")
    
Wrap the output of a function in a Tensor object.
Parameters:
    
  * **args** (_Any_) â Variable length argument list
  * **kwargs** (_Any_) â Arbitrary keyword arguments


Returns:
    
Tensor object
Return type:
    
[Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.Tensor")
convolution(_* args:Any_, _** kwargs:Any_) â [Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.tensor.Tensor")[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.NNFactory.convolution> "Link to this definition")
    
Wrap the output of a function in a Tensor object.
Parameters:
    
  * **args** (_Any_) â Variable length argument list
  * **kwargs** (_Any_) â Arbitrary keyword arguments


Returns:
    
Tensor object
Return type:
    
[Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.Tensor")
get_backend_dtype(_dtype_) â c_char_p[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.NNFactory.get_backend_dtype> "Link to this definition")
    
Get the string representation of the dtype.
Parameters:
    
**dtype** â numpy dtype
Raises:
    
**RuntimeError** â Unsupported datatype
Returns:
    
string representation of the dtype
Return type:
    
ctypes.c_char_p
get_tensor_dtype(_node_)[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.NNFactory.get_tensor_dtype> "Link to this definition")
    
Get tensor dtype.
Parameters:
    
**node** â network node
Raises:
    
**RuntimeError** â Unsupported dtype
Returns:
    
tensor dtype
Return type:
    
str
get_tensor_recursively(_args :Sequence[Any]_) â List[ndarray][#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.NNFactory.get_tensor_recursively> "Link to this definition")
    
Get tensor recursively for a list of arguments.
Parameters:
    
**args** (_Sequence_ _[__Any_ _]_) â Sequence of tensors, tuple of tensors and additional arguments
Returns:
    
Sequence of tensors
Return type:
    
List[np.ndarray]
get_tensor_shape(_node_)[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.NNFactory.get_tensor_shape> "Link to this definition")
    
Get tensor shape.
Parameters:
    
**node** â network node
Returns:
    
tensor shape
Return type:
    
tuple[int]
linear(_* args:Any_, _** kwargs:Any_) â [Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.tensor.Tensor")[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.NNFactory.linear> "Link to this definition")
    
Wrap the output of a function in a Tensor object.
Parameters:
    
  * **args** (_Any_) â Variable length argument list
  * **kwargs** (_Any_) â Arbitrary keyword arguments


Returns:
    
Tensor object
Return type:
    
[Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.Tensor")
matmul(_* args:Any_, _** kwargs:Any_) â [Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.tensor.Tensor")[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.NNFactory.matmul> "Link to this definition")
    
Wrap the output of a function in a Tensor object.
Parameters:
    
  * **args** (_Any_) â Variable length argument list
  * **kwargs** (_Any_) â Arbitrary keyword arguments


Returns:
    
Tensor object
Return type:
    
[Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.Tensor")
max_pooling(_* args:Any_, _** kwargs:Any_) â [Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.tensor.Tensor")[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.NNFactory.max_pooling> "Link to this definition")
    
Wrap the output of a function in a Tensor object.
Parameters:
    
  * **args** (_Any_) â Variable length argument list
  * **kwargs** (_Any_) â Arbitrary keyword arguments


Returns:
    
Tensor object
Return type:
    
[Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.Tensor")
normL2(_* args:Any_, _** kwargs:Any_) â [Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.tensor.Tensor")[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.NNFactory.normL2> "Link to this definition")
    
Wrap the output of a function in a Tensor object.
Parameters:
    
  * **args** (_Any_) â Variable length argument list
  * **kwargs** (_Any_) â Arbitrary keyword arguments


Returns:
    
Tensor object
Return type:
    
[Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.Tensor")
parameter(_* args:Any_, _** kwargs:Any_) â [Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.tensor.Tensor")[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.NNFactory.parameter> "Link to this definition")
    
Wrap the output of a function in a Tensor object.
Parameters:
    
  * **args** (_Any_) â Variable length argument list
  * **kwargs** (_Any_) â Arbitrary keyword arguments


Returns:
    
Tensor object
Return type:
    
[Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.Tensor")
power(_* args:Any_, _** kwargs:Any_) â [Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.tensor.Tensor")[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.NNFactory.power> "Link to this definition")
    
Wrap the output of a function in a Tensor object.
Parameters:
    
  * **args** (_Any_) â Variable length argument list
  * **kwargs** (_Any_) â Arbitrary keyword arguments


Returns:
    
Tensor object
Return type:
    
[Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.Tensor")
reduce_max(_* args:Any_, _** kwargs:Any_) â [Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.tensor.Tensor")[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.NNFactory.reduce_max> "Link to this definition")
    
Wrap the output of a function in a Tensor object.
Parameters:
    
  * **args** (_Any_) â Variable length argument list
  * **kwargs** (_Any_) â Arbitrary keyword arguments


Returns:
    
Tensor object
Return type:
    
[Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.Tensor")
reduce_mean(_* args:Any_, _** kwargs:Any_) â [Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.tensor.Tensor")[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.NNFactory.reduce_mean> "Link to this definition")
    
Wrap the output of a function in a Tensor object.
Parameters:
    
  * **args** (_Any_) â Variable length argument list
  * **kwargs** (_Any_) â Arbitrary keyword arguments


Returns:
    
Tensor object
Return type:
    
[Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.Tensor")
reduce_min(_* args:Any_, _** kwargs:Any_) â [Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.tensor.Tensor")[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.NNFactory.reduce_min> "Link to this definition")
    
Wrap the output of a function in a Tensor object.
Parameters:
    
  * **args** (_Any_) â Variable length argument list
  * **kwargs** (_Any_) â Arbitrary keyword arguments


Returns:
    
Tensor object
Return type:
    
[Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.Tensor")
reduce_prod(_* args:Any_, _** kwargs:Any_) â [Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.tensor.Tensor")[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.NNFactory.reduce_prod> "Link to this definition")
    
Wrap the output of a function in a Tensor object.
Parameters:
    
  * **args** (_Any_) â Variable length argument list
  * **kwargs** (_Any_) â Arbitrary keyword arguments


Returns:
    
Tensor object
Return type:
    
[Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.Tensor")
reduce_sum(_* args:Any_, _** kwargs:Any_) â [Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.tensor.Tensor")[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.NNFactory.reduce_sum> "Link to this definition")
    
Wrap the output of a function in a Tensor object.
Parameters:
    
  * **args** (_Any_) â Variable length argument list
  * **kwargs** (_Any_) â Arbitrary keyword arguments


Returns:
    
Tensor object
Return type:
    
[Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.Tensor")
reshape(_* args:Any_, _** kwargs:Any_) â [Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.tensor.Tensor")[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.NNFactory.reshape> "Link to this definition")
    
Wrap the output of a function in a Tensor object.
Parameters:
    
  * **args** (_Any_) â Variable length argument list
  * **kwargs** (_Any_) â Arbitrary keyword arguments


Returns:
    
Tensor object
Return type:
    
[Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.Tensor")
return_tensor() â F[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.NNFactory.return_tensor> "Link to this definition")
    
Wrap the output of a function in a Tensor object.
Parameters:
    
**fn** (_function_) â Function
Returns:
    
A function that wraps the output in a Tensor object
Return type:
    
function
run(_X :ndarray_, _* weights:ndarray|Tuple[ndarray,ndarray]_, _** kwargs:Any_) â ndarray[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.NNFactory.run> "Link to this definition")
    
Run the layer: X * W^T.
Parameters:
    
  * **X** (_np.ndarray_) â lhs operator
  * **weights** (_Union_ _[__np.ndarray_ _,__Tuple_ _[__np.ndarray_ _,__np.ndarray_ _]__]_) â rhs operators
  * **kwargs** (_Any_) â additional arguments


Returns:
    
result
Return type:
    
np.ndarray
set_input_tensor(_tensor :ndarray_, _idx :int_)[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.NNFactory.set_input_tensor> "Link to this definition")
    
Set input tensor.
Parameters:
    
  * **tensor** (_np.ndarray_) â Input tensor
  * **idx** (_int_) â tensor index


slice(_* args:Any_, _** kwargs:Any_) â [Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.tensor.Tensor")[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.NNFactory.slice> "Link to this definition")
    
Wrap the output of a function in a Tensor object.
Parameters:
    
  * **args** (_Any_) â Variable length argument list
  * **kwargs** (_Any_) â Arbitrary keyword arguments


Returns:
    
Tensor object
Return type:
    
[Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.Tensor")
to(_* args:Any_, _** kwargs:Any_) â [Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.tensor.Tensor")[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.NNFactory.to> "Link to this definition")
    
Wrap the output of a function in a Tensor object.
Parameters:
    
  * **args** (_Any_) â Variable length argument list
  * **kwargs** (_Any_) â Arbitrary keyword arguments


Returns:
    
Tensor object
Return type:
    
[Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.Tensor")
transpose(_* args:Any_, _** kwargs:Any_) â [Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.tensor.Tensor")[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.NNFactory.transpose> "Link to this definition")
    
Wrap the output of a function in a Tensor object.
Parameters:
    
  * **args** (_Any_) â Variable length argument list
  * **kwargs** (_Any_) â Arbitrary keyword arguments


Returns:
    
Tensor object
Return type:
    
[Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.Tensor")
unsqueeze(_* args:Any_, _** kwargs:Any_) â [Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.tensor.Tensor")[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.NNFactory.unsqueeze> "Link to this definition")
    
Wrap the output of a function in a Tensor object.
Parameters:
    
  * **args** (_Any_) â Variable length argument list
  * **kwargs** (_Any_) â Arbitrary keyword arguments


Returns:
    
Tensor object
Return type:
    
[Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.Tensor")
_class_ intel_npu_acceleration_library.backend.QLinear(_inC: int_, _outC: int_, _batch: int_, _profile: bool = False_, _device: str = 'NPU'_, _dtype: ~numpy.dtype = <class 'numpy.int8'>_)[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.QLinear> "Link to this definition")
    
Bases: `[NNFactory`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.factory.NNFactory> "intel_npu_acceleration_library.backend.factory.NNFactory")
Quantized Linear class, computing a matrix matrix multiplication with weights prefetching.
run(_X :ndarray_, _W :ndarray_, _scale :ndarray_, _op_id :str_) â ndarray[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.QLinear.run> "Link to this definition")
    
Run the layer: $X * (W * S)^T$ .
Parameters:
    
  * **X** (_np.ndarray_) â activation
  * **W** (_np.ndarray_) â quantized weights
  * **scale** (_np.ndarray_) â quantization scale
  * **op_id** (_str_) â operation id


Raises:
    
**RuntimeError** â Input, weights or scale shape mismatch
Returns:
    
result
Return type:
    
np.ndarray
_class_ intel_npu_acceleration_library.backend.QMatMul(_inC: int_, _outC: int_, _batch: int_, _profile: bool = False_, _device: str = 'NPU'_, _dtype: ~numpy.dtype = <class 'numpy.int8'>_)[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.QMatMul> "Link to this definition")
    
Bases: `[NNFactory`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.factory.NNFactory> "intel_npu_acceleration_library.backend.factory.NNFactory")
Quantized Linear class, computing a matrix matrix multiplication.
run(_X :ndarray_, _W :ndarray_, _scale :ndarray_) â ndarray[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.QMatMul.run> "Link to this definition")
    
Run the layer: X * (W * S)^T.
Parameters:
    
  * **X** (_np.ndarray_) â activation
  * **W** (_np.ndarray_) â quantized weights
  * **scale** (_np.ndarray_) â quantization scale


Raises:
    
**RuntimeError** â Input, weights or scale shape mismatch
Returns:
    
result
Return type:
    
np.ndarray
_class_ intel_npu_acceleration_library.backend.SDPA(_query_shapes :Tuple[int,int]_, _key_shapes :Tuple[int,int]_, _value_shapes :Tuple[int,int]_, _mask_shapes :Tuple[int,int]_, _is_causal :bool=False_, _profile :bool=False_, _device :str='NPU'_)[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.SDPA> "Link to this definition")
    
Bases: `[NNFactory`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.factory.NNFactory> "intel_npu_acceleration_library.backend.factory.NNFactory")
Implementation of a ScaledDotProductAttention NPU operation.
run(_query :ndarray_, _key :ndarray_, _value :ndarray_, _mask :ndarray_) â ndarray[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.SDPA.run> "Link to this definition")
    
Run the scaled dot product attention kernel.
Parameters:
    
  * **query** (_np.ndarray_) â sdpa query tensor
  * **key** (_np.ndarray_) â sdpa key tensor
  * **value** (_np.ndarray_) â sdpa value tensor
  * **mask** (_np.ndarray_) â sdpa mask tensor


Returns:
    
result
Return type:
    
np.ndarray
_class_ intel_npu_acceleration_library.backend.SimpleSDPA(_query_shapes :Tuple[int,int]_, _key_shapes :Tuple[int,int]_, _value_shapes :Tuple[int,int]_, _is_causal :bool=False_, _profile :bool=False_, _device :str='NPU'_)[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.SimpleSDPA> "Link to this definition")
    
Bases: `[NNFactory`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.factory.NNFactory> "intel_npu_acceleration_library.backend.factory.NNFactory")
Implementation of a ScaledDotProductAttention NPU operation.
run(_query :ndarray_, _key :ndarray_, _value :ndarray_) â ndarray[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.SimpleSDPA.run> "Link to this definition")
    
Run the scaled dot product attention kernel.
Parameters:
    
  * **query** (_np.ndarray_) â sdpa query tensor
  * **key** (_np.ndarray_) â sdpa key tensor
  * **value** (_np.ndarray_) â sdpa value tensor


Returns:
    
result
Return type:
    
np.ndarray
_class_ intel_npu_acceleration_library.backend.Tensor(_factory :[NNFactory](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.NNFactory> "intel_npu_acceleration_library.backend.NNFactory")_, _node :_Pointer_)[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "Link to this definition")
    
Bases: `object`
Represents a tensor object.
Attrs:
    
factory (NNFactory): The factory object used to create the tensor. node (ctypes._Pointer): The pointer to the underlying tensor node. shape (Sequence[int]): The shape of the tensor. dtype (NPUDtype): The data type of the tensor. T (Tensor): The transpose of the tensor.
__add__(_self_ , _other_)[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.__add__> "Link to this definition")
    
Adds two tensors element-wise.
__sub__(_self_ , _other_)[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.__sub__> "Link to this definition")
    
Subtracts two tensors element-wise.
__mul__(_self_ , _other_)[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.__mul__> "Link to this definition")
    
Multiplies two tensors element-wise.
__truediv__(_self_ , _other_)[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.__truediv__> "Link to this definition")
    
Divides two tensors element-wise.
__neg__(_self_)[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.__neg__> "Link to this definition")
    
Negates the tensor.
__repr__(_self_)[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.__repr__> "Link to this definition")
    
Returns a string representation of the tensor.
__str__(_self_)[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.__str__> "Link to this definition")
    
Returns a string representation of the tensor.
__len__(_self_)[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.__len__> "Link to this definition")
    
Returns the total number of elements in the tensor.
T()[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.T> "Link to this definition")
    
Returns the transpose of the tensor.
squeeze(_self_)[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.squeeze> "Link to this definition")
    
Removes dimensions of size 1 from the tensor.
unsqueeze(_self_ , _axis_)[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.unsqueeze> "Link to this definition")
    
Adds a dimension of size 1 to the tensor.
__matmul__(_self_ , _other_)[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.__matmul__> "Link to this definition")
    
Performs matrix multiplication between two tensors.
acos(_self_)[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.acos> "Link to this definition")
    
Applies acos function to the tensor.
asin(_self_)[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.asin> "Link to this definition")
    
Applies asin function to the tensor.
atan(_self_)[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.atan> "Link to this definition")
    
Applies atan function to the tensor.
acosh(_self_)[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.acosh> "Link to this definition")
    
Applies acosh function to the tensor.
asinh(_self_)[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.asinh> "Link to this definition")
    
Applies asinh function to the tensor.
atanh(_self_)[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.atanh> "Link to this definition")
    
Applies atanh function to the tensor.
cosh(_self_)[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.cosh> "Link to this definition")
    
Applies cosh function to the tensor.
sinh(_self_)[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.sinh> "Link to this definition")
    
Applies sinh function to the tensor.
tanh(_self_)[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.tanh> "Link to this definition")
    
Applies tanh function to the tensor.
cos(_self_)[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.cos> "Link to this definition")
    
Applies cos function to the tensor.
sin(_self_)[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.sin> "Link to this definition")
    
Applies sin function to the tensor.
tan(_self_)[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.tan> "Link to this definition")
    
Applies tan function to the tensor.
ceiling(_self_)[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.ceiling> "Link to this definition")
    
Applies ceil function to the tensor.
clamp(_self_ , _min_ , _max_)[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.clamp> "Link to this definition")
    
Applies clamp function to the tensor.
elu(_self_ , _alpha_)[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.elu> "Link to this definition")
    
Applies elu function to the tensor.
erf(_self_)[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.erf> "Link to this definition")
    
Applies erf function to the tensor.
exp(_self_)[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.exp> "Link to this definition")
    
Applies exponental function to the tensor.
floor(_self_)[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.floor> "Link to this definition")
    
Applies floor function to the tensor.
grn(_self_ , _bias_)[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.grn> "Link to this definition")
    
Applies grn function to the tensor.
hsigmoid(_self_)[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.hsigmoid> "Link to this definition")
    
Applies hsigmoid function to the tensor.
hswish(_self_)[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.hswish> "Link to this definition")
    
Applies hswish function to the tensor.
log(_self_)[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.log> "Link to this definition")
    
Applies log function to the tensor.
mish(_self_)[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.mish> "Link to this definition")
    
Applies mish function to the tensor.
relu(_self_ , _bias_)[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.relu> "Link to this definition")
    
Applies relu function to the tensor.
round(_self_)[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.round> "Link to this definition")
    
Applies round function to the tensor.
sigmoid(_self_)[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.sigmoid> "Link to this definition")
    
Applies sigmoid function to the tensor.
sign(_self_)[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.sign> "Link to this definition")
    
Applies sign function to the tensor.
softmax(_self_ , _dim_)[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.softmax> "Link to this definition")
    
Applies softmax function to the tensor.
softplus(_self_)[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.softplus> "Link to this definition")
    
Applies softplus function to the tensor.
sqrt(_self_)[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.sqrt> "Link to this definition")
    
Applies sqrt function to the tensor.
max(_self_ , _dim_ , _keep_dims_)[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.max> "Link to this definition")
    
Returns the reduced max tensor.
mean(_self_ , _dim_ , _keep_dims_ , _dtype_)[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.mean> "Link to this definition")
    
Returns the reduced mean tensor.
min(_self_ , _dim_ , _keep_dims_)[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.min> "Link to this definition")
    
Returns the reduced min tensor.
prod(_self_ , _dim_ , _keep_dims_ , _dtype_)[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.prod> "Link to this definition")
    
Returns the reduced product tensor.
sum(_self_ , _dim_ , _keep_dims_ , _dtype_)[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.sum> "Link to this definition")
    
Returns the reduced sum tensor.
_property_ T _:[ Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.tensor.Tensor")_[#](https://intel.github.io/intel-npu-acceleration-library/python/<#id0> "Link to this definition")
    
Return the transpose of the tensor.
Returns:
    
The transposed tensor.
Return type:
    
[Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.Tensor")
acos() â [Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.tensor.Tensor")[#](https://intel.github.io/intel-npu-acceleration-library/python/<#id1> "Link to this definition")
    
Apply the acos function to the tensor.
Returns:
    
The result of applying the acos function.
Return type:
    
[Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.Tensor")
acosh() â [Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.tensor.Tensor")[#](https://intel.github.io/intel-npu-acceleration-library/python/<#id2> "Link to this definition")
    
Apply the acosh function to the tensor.
Returns:
    
The result of applying the acosh function.
Return type:
    
[Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.Tensor")
asin() â [Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.tensor.Tensor")[#](https://intel.github.io/intel-npu-acceleration-library/python/<#id3> "Link to this definition")
    
Apply the asin function to the tensor.
Returns:
    
The result of applying the asin function.
Return type:
    
[Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.Tensor")
asinh() â [Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.tensor.Tensor")[#](https://intel.github.io/intel-npu-acceleration-library/python/<#id4> "Link to this definition")
    
Apply the asinh function to the tensor.
Returns:
    
The result of applying the asinh function.
Return type:
    
[Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.Tensor")
atan() â [Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.tensor.Tensor")[#](https://intel.github.io/intel-npu-acceleration-library/python/<#id5> "Link to this definition")
    
Apply the atan function to the tensor.
Returns:
    
The result of applying the atan function.
Return type:
    
[Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.Tensor")
atanh() â [Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.tensor.Tensor")[#](https://intel.github.io/intel-npu-acceleration-library/python/<#id6> "Link to this definition")
    
Apply the atanh function to the tensor.
Returns:
    
The result of applying the atanh function.
Return type:
    
[Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.Tensor")
ceiling() â [Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.tensor.Tensor")[#](https://intel.github.io/intel-npu-acceleration-library/python/<#id7> "Link to this definition")
    
Apply the ceiling function to the tensor.
Returns:
    
The result of applying the ceiling function.
Return type:
    
[Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.Tensor")
chunk(_chunks :int_, _dim :int=0_) â [Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.tensor.Tensor")|list[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.chunk> "Link to this definition")
    
Return the list of tensor chunks.
Parameters:
    
  * **chunks** (_int_) â The number of chunks to return.
  * **dim** (_int_) â The dimension along which to split the tensor. Default is 0.


Returns:
    
The resulting list of split tensors or a single tensor.
Return type:
    
Union[âTensorâ, list]
Raises:
    
**ValueError** â The input chunks value is not valid.
clamp(_min =None_, _max =None_) â [Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.tensor.Tensor")[#](https://intel.github.io/intel-npu-acceleration-library/python/<#id8> "Link to this definition")
    
Apply the clamp function to the tensor.
Parameters:
    
  * **min** (_int_ _,__float_) â The lower-bound of the range to be clamped
  * **max** (_int_ _,__float_) â The upper-bound of the range to be clamped


Returns:
    
The result of applying the ceil function.
Return type:
    
[Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.Tensor")
cos() â [Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.tensor.Tensor")[#](https://intel.github.io/intel-npu-acceleration-library/python/<#id9> "Link to this definition")
    
Apply the cos function to the tensor.
Returns:
    
The result of applying the cos function.
Return type:
    
[Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.Tensor")
cosh() â [Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.tensor.Tensor")[#](https://intel.github.io/intel-npu-acceleration-library/python/<#id10> "Link to this definition")
    
Apply the cosh function to the tensor.
Returns:
    
The result of applying the cosh function.
Return type:
    
[Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.Tensor")
dim() â int[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.dim> "Link to this definition")
    
Return the number of dimensions of the tensor.
Returns:
    
The number of dimensions of the tensor.
Return type:
    
int
_property_ dtype _: NPUDtype_[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.dtype> "Link to this definition")
    
Returns the data type of the tensor.
Returns:
    
The data type of the tensor.
Return type:
    
type
elu(_alpha :float=1.0_) â [Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.tensor.Tensor")[#](https://intel.github.io/intel-npu-acceleration-library/python/<#id11> "Link to this definition")
    
Apply the elu function to the tensor.
Parameters:
    
**alpha** (_float_) â The alpha value. Defaults to 1.0.
Returns:
    
The result of applying the elu function.
Return type:
    
[Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.Tensor")
erf() â [Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.tensor.Tensor")[#](https://intel.github.io/intel-npu-acceleration-library/python/<#id12> "Link to this definition")
    
Apply the erf function to the tensor.
Returns:
    
The result of applying the erf function.
Return type:
    
[Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.Tensor")
exp() â [Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.tensor.Tensor")[#](https://intel.github.io/intel-npu-acceleration-library/python/<#id13> "Link to this definition")
    
Apply the exp function to the tensor.
Returns:
    
The result of applying the exp function.
Return type:
    
[Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.Tensor")
factory _:[ NNFactory](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.NNFactory> "intel_npu_acceleration_library.backend.NNFactory")_[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.factory> "Link to this definition")
flatten(_start_dim =0_, _end_dim =-1_) â [Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.tensor.Tensor")[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.flatten> "Link to this definition")
    
Flatten the tensor.
Parameters:
    
  * **start_dim** (_int_) â The first dim to flatten. Defaults to 0.
  * **end_dim** (_int_) â The last dim to flatten. Defaults to -1.


Returns:
    
The flattened tensor.
Return type:
    
[Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.Tensor")
floor() â [Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.tensor.Tensor")[#](https://intel.github.io/intel-npu-acceleration-library/python/<#id14> "Link to this definition")
    
Apply the floor function to the tensor.
Returns:
    
The result of applying the floor function.
Return type:
    
[Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.Tensor")
grn(_bias :float=1e-12_) â [Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.tensor.Tensor")[#](https://intel.github.io/intel-npu-acceleration-library/python/<#id15> "Link to this definition")
    
Apply the grn function to the tensor.
Parameters:
    
**bias** (_float_) â The bias value. Defaults to 1e-12.
Returns:
    
The result of applying the grn function.
Return type:
    
[Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.Tensor")
hsigmoid() â [Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.tensor.Tensor")[#](https://intel.github.io/intel-npu-acceleration-library/python/<#id16> "Link to this definition")
    
Apply the hsigmoid function to the tensor.
Returns:
    
The result of applying the hsigmoid function.
Return type:
    
[Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.Tensor")
hswish() â [Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.tensor.Tensor")[#](https://intel.github.io/intel-npu-acceleration-library/python/<#id17> "Link to this definition")
    
Apply the hswish function to the tensor.
Returns:
    
The result of applying the hswish function.
Return type:
    
[Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.Tensor")
log() â [Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.tensor.Tensor")[#](https://intel.github.io/intel-npu-acceleration-library/python/<#id18> "Link to this definition")
    
Apply the log function to the tensor.
Returns:
    
The result of applying the log function.
Return type:
    
[Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.Tensor")
max(_dim :int|None=None_, _keep_dims :bool|None=False_) â [Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.tensor.Tensor")[#](https://intel.github.io/intel-npu-acceleration-library/python/<#id19> "Link to this definition")
    
Return the reduced max tensor.
Parameters:
    
  * **dim** (_Optional_ _[__int_ _]__,__optional_) â The dim to reduce. Default is None, and all dimensions are reduced.
  * **keep_dims** (_Optional_ _[__bool_ _]__,__optional_) â If set to 1 it holds axes that are used for reduction. Defaults to False.


Returns:
    
The result of max reducing operation.
Return type:
    
[Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.Tensor")
mean(_dim :int|Sequence[int]|None=None_, _keep_dims :bool|None=False_, _dtype :dtype|None=None_) â [Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.tensor.Tensor")[#](https://intel.github.io/intel-npu-acceleration-library/python/<#id20> "Link to this definition")
    
Return the reduced mean tensor.
Parameters:
    
  * **dim** (_Optional_ _[__Union_ _[__int_ _,__Sequence_ _[__int_ _]__]__]__,__optional_) â The dim(s) to reduce. Default is None, and all dimensions are reduced.
  * **keep_dims** (_Optional_ _[__bool_ _]__,__optional_) â If set to 1 it holds axes that are used for reduction. Defaults to False.
  * **dtype** (_Optional_ _[__torch.dtype_ _]__,__optional_) â The data type. Defaults to None.


Returns:
    
The result of mean reducing operation.
Return type:
    
[Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.Tensor")
min(_dim :int|None=None_, _keep_dims :bool|None=False_) â [Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.tensor.Tensor")[#](https://intel.github.io/intel-npu-acceleration-library/python/<#id21> "Link to this definition")
    
Return the reduced min tensor.
Parameters:
    
  * **dim** (_Optional_ _[__int_ _]__,__optional_) â The dim to reduce. Default is None, and all dimensions are reduced.
  * **keep_dims** (_Optional_ _[__bool_ _]__,__optional_) â If set to 1 it holds axes that are used for reduction. Defaults to False.


Returns:
    
The result of min reducing operation.
Return type:
    
[Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.Tensor")
mish() â [Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.tensor.Tensor")[#](https://intel.github.io/intel-npu-acceleration-library/python/<#id22> "Link to this definition")
    
Apply the mish function to the tensor.
Returns:
    
The result of applying the mish function.
Return type:
    
[Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.Tensor")
node _: _Pointer_[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.node> "Link to this definition")
permute(_* input_order:int_) â [Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.tensor.Tensor")[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.permute> "Link to this definition")
    
Return the transpose of the tensor.
Parameters:
    
**input_order** (_Sequence_ _[__int_ _]_) â The order of the dimensions in the transposed tensor.
Returns:
    
The transposed tensor.
Return type:
    
[Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.Tensor")
prod(_dim :int|None=None_, _keep_dims :bool|None=False_, _dtype :dtype|None=None_) â [Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.tensor.Tensor")[#](https://intel.github.io/intel-npu-acceleration-library/python/<#id23> "Link to this definition")
    
Return the reduced product tensor.
Parameters:
    
  * **dim** (_Optional_ _[__int_ _]__,__optional_) â The dim to reduce. Default is None, and all dimensions are reduced.
  * **keep_dims** (_Optional_ _[__bool_ _]__,__optional_) â If set to 1 it holds axes that are used for reduction. Defaults to False.
  * **dtype** (_Optional_ _[__torch.dtype_ _]__,__optional_) â The data type. Defaults to None.


Returns:
    
The result of product reducing operation.
Return type:
    
[Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.Tensor")
relu() â [Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.tensor.Tensor")[#](https://intel.github.io/intel-npu-acceleration-library/python/<#id24> "Link to this definition")
    
Apply the relu function to the tensor.
Returns:
    
The result of applying the relu function.
Return type:
    
[Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.Tensor")
reshape(_* shape:int|Sequence[int]_) â [Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.tensor.Tensor")[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.reshape> "Link to this definition")
    
Return the transpose of the tensor.
Parameters:
    
**shape** (_Union_ _[__int_ _,__Sequence_ _[__int_ _]__]_) â The new shape of the tensor.
Returns:
    
The transposed tensor.
Return type:
    
[Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.Tensor")
round() â [Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.tensor.Tensor")[#](https://intel.github.io/intel-npu-acceleration-library/python/<#id25> "Link to this definition")
    
Apply the round function to the tensor.
Returns:
    
The result of applying the round function.
Return type:
    
[Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.Tensor")
_property_ shape _: Sequence[int]_[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.shape> "Link to this definition")
    
Returns the shape of the tensor.
Returns:
    
The shape of the tensor.
Return type:
    
Sequence[int]
sigmoid() â [Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.tensor.Tensor")[#](https://intel.github.io/intel-npu-acceleration-library/python/<#id26> "Link to this definition")
    
Apply the sigmoid function to the tensor.
Returns:
    
The result of applying the sigmoid function.
Return type:
    
[Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.Tensor")
sign() â [Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.tensor.Tensor")[#](https://intel.github.io/intel-npu-acceleration-library/python/<#id27> "Link to this definition")
    
Apply the sign function to the tensor.
Returns:
    
The result of applying the sign function.
Return type:
    
[Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.Tensor")
sin() â [Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.tensor.Tensor")[#](https://intel.github.io/intel-npu-acceleration-library/python/<#id28> "Link to this definition")
    
Apply the sin function to the tensor.
Returns:
    
The result of applying the sin function.
Return type:
    
[Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.Tensor")
sinh() â [Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.tensor.Tensor")[#](https://intel.github.io/intel-npu-acceleration-library/python/<#id29> "Link to this definition")
    
Apply the sinh function to the tensor.
Returns:
    
The result of applying the sinh function.
Return type:
    
[Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.Tensor")
size(_dim =None_) â int|Sequence[int][#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.size> "Link to this definition")
    
Return the size of the tensor.
Parameters:
    
**dim** (_int_ _,__optional_) â The dimension to return the size of. Defaults to None.
Returns:
    
The size of the tensor.
Return type:
    
Union[int, Sequence[int]]
softmax(_dim_) â [Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.tensor.Tensor")[#](https://intel.github.io/intel-npu-acceleration-library/python/<#id30> "Link to this definition")
    
Apply the softmax function to the tensor.
Parameters:
    
**dim** (_int_) â The dimension to apply softmax.
Returns:
    
The result of applying the softmax function.
Return type:
    
[Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.Tensor")
softplus() â [Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.tensor.Tensor")[#](https://intel.github.io/intel-npu-acceleration-library/python/<#id31> "Link to this definition")
    
Apply the softplus function to the tensor.
Returns:
    
The result of applying the softplus function.
Return type:
    
[Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.Tensor")
sqrt() â [Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.tensor.Tensor")[#](https://intel.github.io/intel-npu-acceleration-library/python/<#id32> "Link to this definition")
    
Apply the sqrt function to the tensor.
Returns:
    
The result of applying the sqrt function.
Return type:
    
[Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.Tensor")
squeeze() â [Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.tensor.Tensor")[#](https://intel.github.io/intel-npu-acceleration-library/python/<#id33> "Link to this definition")
    
Remove dimensions of size 1 from the tensor.
Returns:
    
The squeezed tensor.
Return type:
    
[Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.Tensor")
sum(_dim :int|Sequence[int]|None=None_, _keep_dims :bool|None=False_, _dtype :dtype|None=None_) â [Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.tensor.Tensor")[#](https://intel.github.io/intel-npu-acceleration-library/python/<#id34> "Link to this definition")
    
Return the reduced sum tensor.
Parameters:
    
  * **dim** (_Optional_ _[__Union_ _[__int_ _,__Sequence_ _[__int_ _]__]__]__,__optional_) â The dim(s) to reduce. Default is None, and all dimensions are reduced.
  * **keep_dims** (_Optional_ _[__bool_ _]__,__optional_) â If set to 1 it holds axes that are used for reduction. Defaults to False.
  * **dtype** (_Optional_ _[__torch.dtype_ _]__,__optional_) â The data type. Defaults to None.


Returns:
    
The result of sum reducing operation.
Return type:
    
[Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.Tensor")
tan() â [Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.tensor.Tensor")[#](https://intel.github.io/intel-npu-acceleration-library/python/<#id35> "Link to this definition")
    
Apply the tan function to the tensor.
Returns:
    
The result of applying the tan function.
Return type:
    
[Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.Tensor")
tanh() â [Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.tensor.Tensor")[#](https://intel.github.io/intel-npu-acceleration-library/python/<#id36> "Link to this definition")
    
Apply the tanh function to the tensor.
Returns:
    
The result of applying the tanh function.
Return type:
    
[Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.Tensor")
to(_dtype :NPUDtype_) â [Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.tensor.Tensor")[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.to> "Link to this definition")
    
Convert the tensor to the specified data type.
Parameters:
    
**dtype** (_NPUDtype_) â The data type to convert the tensor to.
Returns:
    
The converted tensor.
Return type:
    
[Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.Tensor")
transpose(_dim0 :int_, _dim1 :int_) â [Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.tensor.Tensor")[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.transpose> "Link to this definition")
    
Return the transpose of the tensor.
Parameters:
    
  * **dim0** (_int_) â The first dimension to transpose.
  * **dim1** (_int_) â The second dimension to transpose.


Returns:
    
The transposed tensor.
Return type:
    
[Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.Tensor")
type(_dtype :NPUDtype_) â [Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.tensor.Tensor")[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.type> "Link to this definition")
    
Convert the tensor to the specified data type.
Parameters:
    
**dtype** (_NPUDtype_) â The data type to convert the tensor to.
Returns:
    
The converted tensor.
Return type:
    
[Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.Tensor")
unsqueeze(_axis_) â [Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.tensor.Tensor")[#](https://intel.github.io/intel-npu-acceleration-library/python/<#id37> "Link to this definition")
    
Add a dimension of size 1 to the tensor.
Parameters:
    
**axis** (_int_) â The axis along which to add the dimension.
Returns:
    
The unsqueezed tensor.
Return type:
    
[Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.Tensor")
view(_* shape:Sequence[int]|int_) â [Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.tensor.Tensor")[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.view> "Link to this definition")
    
Return the transpose of the tensor.
Parameters:
    
**shape** (_Union_ _[__Sequence_ _[__int_ _]__,__int_ _]_) â The new shape of the tensor.
Returns:
    
The transposed tensor.
Return type:
    
[Tensor](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor> "intel_npu_acceleration_library.backend.Tensor")
intel_npu_acceleration_library.backend.clear_cache()[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.clear_cache> "Link to this definition")
    
Clear the cache of models.
intel_npu_acceleration_library.backend.get_driver_version() â int[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.get_driver_version> "Link to this definition")
    
Get the driver version for the IntelÂ® NPU Acceleration Library.
Raises:
    
**RuntimeError** â an error is raised if the platform is not supported. Currently supported platforms are Windows and Linux
Returns:
    
NPU driver version
Return type:
    
int
intel_npu_acceleration_library.backend.npu_available() â bool[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.npu_available> "Link to this definition")
    
Return if the NPU is available.
Returns:
    
Return True if the NPU is available in the system
Return type:
    
bool
intel_npu_acceleration_library.backend.run_factory(_x :Tensor|List[Tensor]_, _weights :List[Tensor]_, _backend_cls :Any_, _op_id :str|None=None_) â Tensor[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.run_factory> "Link to this definition")
    
Run a factory operation. Depending on the datatype of the weights it runs a float or quantized operation.
Parameters:
    
  * **x** (_Union_ _[__torch.Tensor_ _,__List_ _[__torch.Tensor_ _]__]_) â Activation tensor(s). Its dtype must be torch.float16
  * **weights** (_torch.Tensor_) â Weights tensor. Its dtype can be torch.float16 or torch.int8
  * **backend_cls** (_Any_) â Backend class to run
  * **op_id** (_Optional_ _[__str_ _]__,__optional_) â Operation ID. Defaults to None.


Returns:
    
result
Return type:
    
torch.Tensor
intel_npu_acceleration_library.backend.run_matmul(_x :Tensor_, _weights :Tensor_, _scale :Tensor|None=None_, _op_id :str|None=None_) â Tensor[#](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.run_matmul> "Link to this definition")
    
Run a matmul operation. Depending on the datatype of the weights it runs a float or quantized operation.
Parameters:
    
  * **x** (_torch.Tensor_) â Activation tensor. Its dtype must be torch.float16
  * **weights** (_torch.Tensor_) â Weights tensor. Its dtype can be torch.float16 or torch.int8
  * **scale** (_Optional_ _[__torch.Tensor_ _]__,__optional_) â Quantization scale. If weights.dtype == torch.int8 then it must be set. Defaults to None.
  * **op_id** (_Optional_ _[__str_ _]__,__optional_) â Operation ID. Defaults to None.


Raises:
    
**RuntimeError** â Unsupported weights datatype. Supported types: [torch.float16, torch.int8]
Returns:
    
result
Return type:
    
torch.Tensor
[ previous intel_npu_acceleration_library package ](https://intel.github.io/intel-npu-acceleration-library/python/<intel_npu_acceleration_library.html> "previous page") [ next intel_npu_acceleration_library.nn package ](https://intel.github.io/intel-npu-acceleration-library/python/<intel_npu_acceleration_library.nn.html> "next page")
Contents 
  * [Submodules](https://intel.github.io/intel-npu-acceleration-library/python/<#submodules>)
  * [intel_npu_acceleration_library.backend.base module](https://intel.github.io/intel-npu-acceleration-library/python/<#intel-npu-acceleration-library-backend-base-module>)
    * `[BaseNPUBackend`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.base.BaseNPUBackend>)
      * `[BaseNPUBackend.save()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.base.BaseNPUBackend.save>)
      * `[BaseNPUBackend.saveCompiledModel()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.base.BaseNPUBackend.saveCompiledModel>)
    * `[BaseNPUBackendWithPrefetch`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.base.BaseNPUBackendWithPrefetch>)
      * `[BaseNPUBackendWithPrefetch.add_to_map()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.base.BaseNPUBackendWithPrefetch.add_to_map>)
      * `[BaseNPUBackendWithPrefetch.create_parameters()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.base.BaseNPUBackendWithPrefetch.create_parameters>)
      * `[BaseNPUBackendWithPrefetch.load_wt_fn()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.base.BaseNPUBackendWithPrefetch.load_wt_fn>)
      * `[BaseNPUBackendWithPrefetch.prefetchWeights()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.base.BaseNPUBackendWithPrefetch.prefetchWeights>)
      * `[BaseNPUBackendWithPrefetch.setWeights()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.base.BaseNPUBackendWithPrefetch.setWeights>)
    * `[adapt_weight()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.base.adapt_weight>)
  * [intel_npu_acceleration_library.backend.factory module](https://intel.github.io/intel-npu-acceleration-library/python/<#intel-npu-acceleration-library-backend-factory-module>)
    * `[NNFactory`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.factory.NNFactory>)
      * `[NNFactory.avg_pooling()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.factory.NNFactory.avg_pooling>)
      * `[NNFactory.compile()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.factory.NNFactory.compile>)
      * `[NNFactory.concat()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.factory.NNFactory.concat>)
      * `[NNFactory.constant()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.factory.NNFactory.constant>)
      * `[NNFactory.convolution()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.factory.NNFactory.convolution>)
      * `[NNFactory.get_backend_dtype()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.factory.NNFactory.get_backend_dtype>)
      * `[NNFactory.get_tensor_dtype()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.factory.NNFactory.get_tensor_dtype>)
      * `[NNFactory.get_tensor_recursively()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.factory.NNFactory.get_tensor_recursively>)
      * `[NNFactory.get_tensor_shape()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.factory.NNFactory.get_tensor_shape>)
      * `[NNFactory.linear()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.factory.NNFactory.linear>)
      * `[NNFactory.matmul()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.factory.NNFactory.matmul>)
      * `[NNFactory.max_pooling()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.factory.NNFactory.max_pooling>)
      * `[NNFactory.normL2()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.factory.NNFactory.normL2>)
      * `[NNFactory.parameter()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.factory.NNFactory.parameter>)
      * `[NNFactory.power()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.factory.NNFactory.power>)
      * `[NNFactory.reduce_max()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.factory.NNFactory.reduce_max>)
      * `[NNFactory.reduce_mean()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.factory.NNFactory.reduce_mean>)
      * `[NNFactory.reduce_min()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.factory.NNFactory.reduce_min>)
      * `[NNFactory.reduce_prod()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.factory.NNFactory.reduce_prod>)
      * `[NNFactory.reduce_sum()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.factory.NNFactory.reduce_sum>)
      * `[NNFactory.reshape()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.factory.NNFactory.reshape>)
      * `[NNFactory.return_tensor()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.factory.NNFactory.return_tensor>)
      * `[NNFactory.run()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.factory.NNFactory.run>)
      * `[NNFactory.set_input_tensor()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.factory.NNFactory.set_input_tensor>)
      * `[NNFactory.slice()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.factory.NNFactory.slice>)
      * `[NNFactory.to()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.factory.NNFactory.to>)
      * `[NNFactory.transpose()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.factory.NNFactory.transpose>)
      * `[NNFactory.unsqueeze()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.factory.NNFactory.unsqueeze>)
  * [intel_npu_acceleration_library.backend.linear module](https://intel.github.io/intel-npu-acceleration-library/python/<#intel-npu-acceleration-library-backend-linear-module>)
    * `[Linear`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.linear.Linear>)
      * `[Linear.run()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.linear.Linear.run>)
  * [intel_npu_acceleration_library.backend.matmul module](https://intel.github.io/intel-npu-acceleration-library/python/<#intel-npu-acceleration-library-backend-matmul-module>)
    * `[MatMul`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.matmul.MatMul>)
      * `[MatMul.run()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.matmul.MatMul.run>)
  * [intel_npu_acceleration_library.backend.mlp module](https://intel.github.io/intel-npu-acceleration-library/python/<#intel-npu-acceleration-library-backend-mlp-module>)
    * `[MLP`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.mlp.MLP>)
  * [intel_npu_acceleration_library.backend.qlinear module](https://intel.github.io/intel-npu-acceleration-library/python/<#intel-npu-acceleration-library-backend-qlinear-module>)
    * `[QLinear`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.qlinear.QLinear>)
      * `[QLinear.run()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.qlinear.QLinear.run>)
  * [intel_npu_acceleration_library.backend.qmatmul module](https://intel.github.io/intel-npu-acceleration-library/python/<#intel-npu-acceleration-library-backend-qmatmul-module>)
    * `[QMatMul`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.qmatmul.QMatMul>)
      * `[QMatMul.run()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.qmatmul.QMatMul.run>)
  * [intel_npu_acceleration_library.backend.runtime module](https://intel.github.io/intel-npu-acceleration-library/python/<#intel-npu-acceleration-library-backend-runtime-module>)
    * `[adapt_output_tensor()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.runtime.adapt_output_tensor>)
    * `[clear_cache()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.runtime.clear_cache>)
    * `[run_factory()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.runtime.run_factory>)
    * `[run_matmul()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.runtime.run_matmul>)
    * `[set_contiguous()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.runtime.set_contiguous>)
  * [Module contents](https://intel.github.io/intel-npu-acceleration-library/python/<#module-intel_npu_acceleration_library.backend>)
    * `[Convolution`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Convolution>)
    * `[Linear`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Linear>)
      * `[Linear.run()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Linear.run>)
    * `[MLP`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.MLP>)
    * `[MatMul`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.MatMul>)
      * `[MatMul.run()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.MatMul.run>)
    * `[NNFactory`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.NNFactory>)
      * `[NNFactory.avg_pooling()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.NNFactory.avg_pooling>)
      * `[NNFactory.compile()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.NNFactory.compile>)
      * `[NNFactory.concat()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.NNFactory.concat>)
      * `[NNFactory.constant()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.NNFactory.constant>)
      * `[NNFactory.convolution()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.NNFactory.convolution>)
      * `[NNFactory.get_backend_dtype()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.NNFactory.get_backend_dtype>)
      * `[NNFactory.get_tensor_dtype()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.NNFactory.get_tensor_dtype>)
      * `[NNFactory.get_tensor_recursively()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.NNFactory.get_tensor_recursively>)
      * `[NNFactory.get_tensor_shape()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.NNFactory.get_tensor_shape>)
      * `[NNFactory.linear()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.NNFactory.linear>)
      * `[NNFactory.matmul()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.NNFactory.matmul>)
      * `[NNFactory.max_pooling()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.NNFactory.max_pooling>)
      * `[NNFactory.normL2()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.NNFactory.normL2>)
      * `[NNFactory.parameter()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.NNFactory.parameter>)
      * `[NNFactory.power()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.NNFactory.power>)
      * `[NNFactory.reduce_max()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.NNFactory.reduce_max>)
      * `[NNFactory.reduce_mean()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.NNFactory.reduce_mean>)
      * `[NNFactory.reduce_min()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.NNFactory.reduce_min>)
      * `[NNFactory.reduce_prod()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.NNFactory.reduce_prod>)
      * `[NNFactory.reduce_sum()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.NNFactory.reduce_sum>)
      * `[NNFactory.reshape()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.NNFactory.reshape>)
      * `[NNFactory.return_tensor()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.NNFactory.return_tensor>)
      * `[NNFactory.run()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.NNFactory.run>)
      * `[NNFactory.set_input_tensor()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.NNFactory.set_input_tensor>)
      * `[NNFactory.slice()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.NNFactory.slice>)
      * `[NNFactory.to()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.NNFactory.to>)
      * `[NNFactory.transpose()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.NNFactory.transpose>)
      * `[NNFactory.unsqueeze()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.NNFactory.unsqueeze>)
    * `[QLinear`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.QLinear>)
      * `[QLinear.run()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.QLinear.run>)
    * `[QMatMul`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.QMatMul>)
      * `[QMatMul.run()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.QMatMul.run>)
    * `[SDPA`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.SDPA>)
      * `[SDPA.run()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.SDPA.run>)
    * `[SimpleSDPA`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.SimpleSDPA>)
      * `[SimpleSDPA.run()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.SimpleSDPA.run>)
    * `[Tensor`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor>)
      * `[Tensor.__add__()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.__add__>)
      * `[Tensor.__sub__()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.__sub__>)
      * `[Tensor.__mul__()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.__mul__>)
      * `[Tensor.__truediv__()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.__truediv__>)
      * `[Tensor.__neg__()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.__neg__>)
      * `[Tensor.__repr__()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.__repr__>)
      * `[Tensor.__str__()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.__str__>)
      * `[Tensor.__len__()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.__len__>)
      * `[Tensor.T()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.T>)
      * `[Tensor.squeeze()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.squeeze>)
      * `[Tensor.unsqueeze()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.unsqueeze>)
      * `[Tensor.__matmul__()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.__matmul__>)
      * `[Tensor.acos()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.acos>)
      * `[Tensor.asin()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.asin>)
      * `[Tensor.atan()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.atan>)
      * `[Tensor.acosh()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.acosh>)
      * `[Tensor.asinh()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.asinh>)
      * `[Tensor.atanh()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.atanh>)
      * `[Tensor.cosh()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.cosh>)
      * `[Tensor.sinh()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.sinh>)
      * `[Tensor.tanh()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.tanh>)
      * `[Tensor.cos()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.cos>)
      * `[Tensor.sin()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.sin>)
      * `[Tensor.tan()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.tan>)
      * `[Tensor.ceiling()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.ceiling>)
      * `[Tensor.clamp()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.clamp>)
      * `[Tensor.elu()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.elu>)
      * `[Tensor.erf()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.erf>)
      * `[Tensor.exp()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.exp>)
      * `[Tensor.floor()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.floor>)
      * `[Tensor.grn()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.grn>)
      * `[Tensor.hsigmoid()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.hsigmoid>)
      * `[Tensor.hswish()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.hswish>)
      * `[Tensor.log()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.log>)
      * `[Tensor.mish()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.mish>)
      * `[Tensor.relu()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.relu>)
      * `[Tensor.round()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.round>)
      * `[Tensor.sigmoid()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.sigmoid>)
      * `[Tensor.sign()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.sign>)
      * `[Tensor.softmax()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.softmax>)
      * `[Tensor.softplus()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.softplus>)
      * `[Tensor.sqrt()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.sqrt>)
      * `[Tensor.max()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.max>)
      * `[Tensor.mean()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.mean>)
      * `[Tensor.min()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.min>)
      * `[Tensor.prod()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.prod>)
      * `[Tensor.sum()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.sum>)
      * `[Tensor.T`](https://intel.github.io/intel-npu-acceleration-library/python/<#id0>)
      * `[Tensor.acos()`](https://intel.github.io/intel-npu-acceleration-library/python/<#id1>)
      * `[Tensor.acosh()`](https://intel.github.io/intel-npu-acceleration-library/python/<#id2>)
      * `[Tensor.asin()`](https://intel.github.io/intel-npu-acceleration-library/python/<#id3>)
      * `[Tensor.asinh()`](https://intel.github.io/intel-npu-acceleration-library/python/<#id4>)
      * `[Tensor.atan()`](https://intel.github.io/intel-npu-acceleration-library/python/<#id5>)
      * `[Tensor.atanh()`](https://intel.github.io/intel-npu-acceleration-library/python/<#id6>)
      * `[Tensor.ceiling()`](https://intel.github.io/intel-npu-acceleration-library/python/<#id7>)
      * `[Tensor.chunk()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.chunk>)
      * `[Tensor.clamp()`](https://intel.github.io/intel-npu-acceleration-library/python/<#id8>)
      * `[Tensor.cos()`](https://intel.github.io/intel-npu-acceleration-library/python/<#id9>)
      * `[Tensor.cosh()`](https://intel.github.io/intel-npu-acceleration-library/python/<#id10>)
      * `[Tensor.dim()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.dim>)
      * `[Tensor.dtype`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.dtype>)
      * `[Tensor.elu()`](https://intel.github.io/intel-npu-acceleration-library/python/<#id11>)
      * `[Tensor.erf()`](https://intel.github.io/intel-npu-acceleration-library/python/<#id12>)
      * `[Tensor.exp()`](https://intel.github.io/intel-npu-acceleration-library/python/<#id13>)
      * `[Tensor.factory`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.factory>)
      * `[Tensor.flatten()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.flatten>)
      * `[Tensor.floor()`](https://intel.github.io/intel-npu-acceleration-library/python/<#id14>)
      * `[Tensor.grn()`](https://intel.github.io/intel-npu-acceleration-library/python/<#id15>)
      * `[Tensor.hsigmoid()`](https://intel.github.io/intel-npu-acceleration-library/python/<#id16>)
      * `[Tensor.hswish()`](https://intel.github.io/intel-npu-acceleration-library/python/<#id17>)
      * `[Tensor.log()`](https://intel.github.io/intel-npu-acceleration-library/python/<#id18>)
      * `[Tensor.max()`](https://intel.github.io/intel-npu-acceleration-library/python/<#id19>)
      * `[Tensor.mean()`](https://intel.github.io/intel-npu-acceleration-library/python/<#id20>)
      * `[Tensor.min()`](https://intel.github.io/intel-npu-acceleration-library/python/<#id21>)
      * `[Tensor.mish()`](https://intel.github.io/intel-npu-acceleration-library/python/<#id22>)
      * `[Tensor.node`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.node>)
      * `[Tensor.permute()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.permute>)
      * `[Tensor.prod()`](https://intel.github.io/intel-npu-acceleration-library/python/<#id23>)
      * `[Tensor.relu()`](https://intel.github.io/intel-npu-acceleration-library/python/<#id24>)
      * `[Tensor.reshape()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.reshape>)
      * `[Tensor.round()`](https://intel.github.io/intel-npu-acceleration-library/python/<#id25>)
      * `[Tensor.shape`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.shape>)
      * `[Tensor.sigmoid()`](https://intel.github.io/intel-npu-acceleration-library/python/<#id26>)
      * `[Tensor.sign()`](https://intel.github.io/intel-npu-acceleration-library/python/<#id27>)
      * `[Tensor.sin()`](https://intel.github.io/intel-npu-acceleration-library/python/<#id28>)
      * `[Tensor.sinh()`](https://intel.github.io/intel-npu-acceleration-library/python/<#id29>)
      * `[Tensor.size()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.size>)
      * `[Tensor.softmax()`](https://intel.github.io/intel-npu-acceleration-library/python/<#id30>)
      * `[Tensor.softplus()`](https://intel.github.io/intel-npu-acceleration-library/python/<#id31>)
      * `[Tensor.sqrt()`](https://intel.github.io/intel-npu-acceleration-library/python/<#id32>)
      * `[Tensor.squeeze()`](https://intel.github.io/intel-npu-acceleration-library/python/<#id33>)
      * `[Tensor.sum()`](https://intel.github.io/intel-npu-acceleration-library/python/<#id34>)
      * `[Tensor.tan()`](https://intel.github.io/intel-npu-acceleration-library/python/<#id35>)
      * `[Tensor.tanh()`](https://intel.github.io/intel-npu-acceleration-library/python/<#id36>)
      * `[Tensor.to()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.to>)
      * `[Tensor.transpose()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.transpose>)
      * `[Tensor.type()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.type>)
      * `[Tensor.unsqueeze()`](https://intel.github.io/intel-npu-acceleration-library/python/<#id37>)
      * `[Tensor.view()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.Tensor.view>)
    * `[clear_cache()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.clear_cache>)
    * `[get_driver_version()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.get_driver_version>)
    * `[npu_available()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.npu_available>)
    * `[run_factory()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.run_factory>)
    * `[run_matmul()`](https://intel.github.io/intel-npu-acceleration-library/python/<#intel_npu_acceleration_library.backend.run_matmul>)


By Intel Corporation 
Â© Copyright 2024, Intel Corporation. 
